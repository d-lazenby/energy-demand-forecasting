{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from src.model import forwardfill_missing_values, make_pipeline, save_model_files\n",
    "from src.paths import PARENT_DIR, MODEL_DIR\n",
    "import src.config as config\n",
    "\n",
    "\n",
    "from src.data import split_data, transform_training_data, prepare_feature_store_data_for_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-19 15:05:22,480 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-19 15:05:22,486 INFO: Initializing external client\n",
      "2025-03-19 15:05:22,486 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-19 15:05:23,863 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1051798\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY,\n",
    ")\n",
    "\n",
    "# connect to feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to feature group\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature view\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, \n",
    "    version=config.FEATURE_VIEW_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (3.54s) \n",
      "2025-03-19 15:12:16,719 INFO: Provenance cached data - overwriting last accessed/created training dataset from 7 to 4.\n"
     ]
    }
   ],
   "source": [
    "# # For making a new training dataset\n",
    "# data, _ = feature_view.training_data(\n",
    "#     description=\"Daily demand\",\n",
    "# )\n",
    "\n",
    "# For obtaining a previous version of the training data\n",
    "data, _ = feature_view.get_training_data(\n",
    "    training_dataset_version=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = prepare_feature_store_data_for_training(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split at 2024-03-01:\n",
      "\tX_train.shape=(62858, 3): 2020-12-01 --- 2024-02-29\n",
      "\ty_train.shape=(62858,)\n",
      "\tX_test.shape=(34556, 3): 2023-03-02 --- 2024-12-12\n",
      "\ty_test.shape=(34556,)\n",
      "Pipeline(steps=[('datetime',\n",
      "                 DatetimeFeatures(drop_original=False,\n",
      "                                  features_to_extract=['month', 'week',\n",
      "                                                       'day_of_week',\n",
      "                                                       'day_of_month',\n",
      "                                                       'weekend'],\n",
      "                                  variables=['datetime'])),\n",
      "                ('lags',\n",
      "                 FeatureEngineerByBA(transformer=LagFeatures(periods=[1, 2, 3,\n",
      "                                                                      4, 5, 6,\n",
      "                                                                      7, 30,\n",
      "                                                                      180,\n",
      "                                                                      365],\n",
      "                                                             variables=['demand']))),\n",
      "                ('windf',\n",
      "                 FeatureEngineerByBA(transformer=WindowFeatures(functions=['mean',\n",
      "                                                                           'std',\n",
      "                                                                           'max',\n",
      "                                                                           'min'],\n",
      "                                                                missing_values='ignore',\n",
      "                                                                variables=['demand'],\n",
      "                                                                window=[3, 7,\n",
      "                                                                        14]))),\n",
      "                ('minmax_scaling', ScaleByBA(scaler=MinMaxScaler())),\n",
      "                ('drop_missing', DropMissingData()),\n",
      "                ('ordinal_enc',\n",
      "                 OrdinalEncoder(encoding_method='arbitrary',\n",
      "                                variables=['ba_code'])),\n",
      "                ('drop_target',\n",
      "                 DropFeatures(features_to_drop=['demand', 'datetime']))])\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000733 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5771\n",
      "[LightGBM] [Info] Number of data points in the train set: 43513, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 211378.464574\n",
      "Training score:\t16675.384962762288\n",
      "Test score:\t19641.851997498266\n"
     ]
    }
   ],
   "source": [
    "# Training pipeline\n",
    "\n",
    "# 0. Constants\n",
    "train_end = \"2024-03-01\"\n",
    "days_of_historic_data = 365\n",
    "\n",
    "# 1. Split Data\n",
    "X_train, y_train, X_test, y_test = split_data(\n",
    "    demand, train_end=train_end, days_of_historic_data=days_of_historic_data\n",
    ")\n",
    "\n",
    "# 2. Transform Data\n",
    "pipe = make_pipeline()\n",
    "print(pipe)\n",
    "\n",
    "X_train_t, y_train_t, X_test_t, y_test_t = transform_training_data(\n",
    "    X_train, y_train, X_test, y_test, pipe\n",
    ")\n",
    "\n",
    "# 3. Train model\n",
    "lgbm = LGBMRegressor()\n",
    "\n",
    "lgbm.fit(X_train_t, y_train_t)\n",
    "\n",
    "# 4. Evaluate model\n",
    "preds_train = lgbm.predict(X_train_t)\n",
    "preds_test = lgbm.predict(X_test_t)\n",
    "\n",
    "mae_train = mean_absolute_error(preds_train, y_train_t)\n",
    "mae_test = mean_absolute_error(preds_test, y_test_t)\n",
    "\n",
    "print(f\"Training score:\\t{mae_train}\")\n",
    "print(f\"Test score:\\t{mae_test}\")\n",
    "\n",
    "# plot_predictions_against_actuals(preds_train, y_train_t, preds_test, y_test_t)\n",
    "# plot_residuals(preds_train, y_train_t, preds_test, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_files(model=lgbm, filename='lgbm.pkl', preprocessing_pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train_t)\n",
    "output_schema = Schema(y_train_t)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_registry = project.get_model_registry()\n",
    "\n",
    "model_entry = model_registry.sklearn.create_model(\n",
    "    name=\"daily_demand_predictor\",\n",
    "    metrics={\"test_mae\": mae_test},\n",
    "    description=\"LightGBM regressor\",\n",
    "    input_example=X_train_t.sample(),\n",
    "    model_schema=model_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/douglaslazenby/Documents/Projects/energy-demand-forecasting/models.zip'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip the model directory\n",
    "shutil.make_archive(PARENT_DIR / \"models\", \"zip\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7230e461104a84b39e1977f77d4b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9a3a2c117d4e829e5f1fa5872441c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/122362 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d9fb7b3e704590b6be7095fc6aa560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/476 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cdb9cfe0c4458f9b4c77a0a5223d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/2385 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1051798/models/daily_demand_predictor/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'daily_demand_predictor', version: 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ZIP = PARENT_DIR / \"models.zip\"\n",
    "\n",
    "model_entry.save(str(MODEL_ZIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
