{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from src.model import forwardfill_missing_values, make_pipeline, save_model\n",
    "from src.paths import MODEL_DIR\n",
    "import src.config as config\n",
    "\n",
    "\n",
    "from src.data import split_data, transform_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-18 13:54:43,468 INFO: Initializing external client\n",
      "2025-03-18 13:54:43,468 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-18 13:54:45,671 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1051798\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# connect to project\n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY,\n",
    ")\n",
    "\n",
    "# connect to feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# connect to feature group\n",
    "feature_group = feature_store.get_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature view if it doesn't exist already\n",
    "try:\n",
    "    # Create feature view if it doesn't exist\n",
    "    feature_store.create_feature_view(\n",
    "        name=config.FEATURE_VIEW_NAME,\n",
    "        version=config.FEATURE_GROUP_VERSION,\n",
    "        query=feature_group.select_all(),  # Create from all features in FG\n",
    "    )\n",
    "except:\n",
    "    print(\"Feature view already existed, skip creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature view\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, version=config.FEATURE_VIEW_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (3.98s) \n",
      "2025-03-18 13:56:48,166 WARNING: VersionWarning: Incremented version to `5`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, _ = feature_view.training_data(\n",
    "    description=\"Daily demand\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_store_data_for_training(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares feature store data for training. \n",
    "\n",
    "    Args:\n",
    "        data: dataframe from Hopsworks feature store\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    from src.config import BAS\n",
    "\n",
    "    data_ = data.copy()\n",
    "\n",
    "    # Filter out unwanted BAs\n",
    "    data_ = data_[data_[\"ba_code\"].isin(BAS)].copy()\n",
    "\n",
    "    data_[\"datetime\"] = pd.to_datetime(data_[\"datetime\"])\n",
    "    data_[\"datetime\"] = data_[\"datetime\"].dt.tz_localize(None)\n",
    "\n",
    "    data_.sort_values(by=[\"ba_code\", \"datetime\"], inplace=True)\n",
    "\n",
    "    data_ = forwardfill_missing_values(data_)\n",
    "\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand = prepare_feature_store_data_for_training(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split at 2024-03-01:\n",
      "\tX_train.shape=(62858, 3): 2020-12-01 --- 2024-02-29\n",
      "\ty_train.shape=(62858,)\n",
      "\tX_test.shape=(39538, 3): 2023-03-02 --- 2025-03-16\n",
      "\ty_test.shape=(39538,)\n",
      "Pipeline(steps=[('datetime',\n",
      "                 DatetimeFeatures(drop_original=False,\n",
      "                                  features_to_extract=['month', 'week',\n",
      "                                                       'day_of_week',\n",
      "                                                       'day_of_month',\n",
      "                                                       'weekend'],\n",
      "                                  variables=['datetime'])),\n",
      "                ('lags',\n",
      "                 FeatureEngineerByBA(transformer=LagFeatures(periods=[1, 2, 3,\n",
      "                                                                      4, 5, 6,\n",
      "                                                                      7, 30,\n",
      "                                                                      180,\n",
      "                                                                      365],\n",
      "                                                             variables=['demand']))),\n",
      "                ('windf',\n",
      "                 FeatureEngineerByBA(transformer=WindowFeatures(functions=['mean',\n",
      "                                                                           'std',\n",
      "                                                                           'max',\n",
      "                                                                           'min'],\n",
      "                                                                missing_values='ignore',\n",
      "                                                                variables=['demand'],\n",
      "                                                                window=[3, 7,\n",
      "                                                                        14]))),\n",
      "                ('minmax_scaling', ScaleByBA(scaler=MinMaxScaler())),\n",
      "                ('drop_missing', DropMissingData()),\n",
      "                ('ordinal_enc',\n",
      "                 OrdinalEncoder(encoding_method='arbitrary',\n",
      "                                variables=['ba_code'])),\n",
      "                ('drop_target',\n",
      "                 DropFeatures(features_to_drop=['demand', 'datetime']))])\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5771\n",
      "[LightGBM] [Info] Number of data points in the train set: 43513, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score 211378.464574\n",
      "Training score:\t16675.384962762288\n",
      "Test score:\t21863.166986104585\n"
     ]
    }
   ],
   "source": [
    "# Training pipeline\n",
    "\n",
    "# 0. Constants\n",
    "train_end = \"2024-03-01\"\n",
    "days_of_historic_data = 365\n",
    "\n",
    "# 1. Split Data\n",
    "X_train, y_train, X_test, y_test = split_data(\n",
    "    demand, train_end=train_end, days_of_historic_data=days_of_historic_data\n",
    ")\n",
    "\n",
    "# 2. Transform Data\n",
    "pipe = make_pipeline()\n",
    "print(pipe)\n",
    "\n",
    "X_train_t, y_train_t, X_test_t, y_test_t = transform_training_data(\n",
    "    X_train, y_train, X_test, y_test, pipe\n",
    ")\n",
    "\n",
    "# 3. Train model\n",
    "lgbm = LGBMRegressor()\n",
    "\n",
    "lgbm.fit(X_train_t, y_train_t)\n",
    "\n",
    "# 4. Evaluate model\n",
    "preds_train = lgbm.predict(X_train_t)\n",
    "preds_test = lgbm.predict(X_test_t)\n",
    "\n",
    "mae_train = mean_absolute_error(preds_train, y_train_t)\n",
    "mae_test = mean_absolute_error(preds_test, y_test_t)\n",
    "\n",
    "print(f\"Training score:\\t{mae_train}\")\n",
    "print(f\"Test score:\\t{mae_test}\")\n",
    "\n",
    "# plot_predictions_against_actuals(preds_train, y_train_t, preds_test, y_test_t)\n",
    "# plot_residuals(preds_train, y_train_t, preds_test, y_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model=lgbm, filename='lgbm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train_t)\n",
    "output_schema = Schema(y_train_t)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb23370fa004d75a62276cacd72311f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c17ada5a94463992ad6231fae812e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/289778 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd892b7ca4c64288b3f714ff24024e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/485 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce930308db74b8ea3d8149aa42882fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/2385 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1051798/models/daily_demand_predictor/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'daily_demand_predictor', version: 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_registry = project.get_model_registry()\n",
    "\n",
    "model = model_registry.sklearn.create_model(\n",
    "    name=\"daily_demand_predictor\",\n",
    "    metrics={\"test_mae\": mae_test},\n",
    "    description=\"LightGBM regressor\",\n",
    "    input_example=X_train_t.sample(),\n",
    "    model_schema=model_schema,\n",
    ")\n",
    "\n",
    "model.save(str(MODEL_DIR / \"lgbm.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
